# Mytorch

Un framework de aprendizaje profundo ligero y educativo, inspirado en PyTorch. Desarrollado desde cero para comprender los fundamentos de las arquitecturas de redes neuronales, la retropropagaciÃ³n y la optimizaciÃ³n.

## ğŸš€ CaracterÃ­sticas

Mytorch implementa los componentes centrales de un stack de deep learning moderno:

- **Capas de Redes Neuronales**:
  - `Linear`: Capas completamente conectadas estÃ¡ndar.
  - `BatchNorm1d` / `BatchNorm2d`: NormalizaciÃ³n por lotes para estabilidad en el entrenamiento.
- **Funciones de ActivaciÃ³n**:
  - `ReLU`, `Sigmoid`, `Tanh`, `Identity`, `GeLU` y `SoftMax`.
- **Funciones de PÃ©rdida**:
  - `MSELoss` para regresiÃ³n.
  - `CrossEntropyLoss` para clasificaciÃ³n.
- **Modelos**:
  - `MLP`: ImplementaciÃ³n modular de PerceptrÃ³n Multicapa.
- **Motor Autograd**: ImplementaciÃ³n personalizada de pasos forward (hacia adelante) y backward (hacia atrÃ¡s).

## ğŸ“ Estructura del Proyecto

```text
HW1P1/
â”œâ”€â”€ mytorch/            # LÃ³gica central del framework
â”‚   â”œâ”€â”€ nn/             # MÃ³dulos de red neuronal (Linear, BatchNorm, etc.)
â”‚   â””â”€â”€ ...            
â”œâ”€â”€ models/             # Arquitecturas de modelos predefinidas (MLP)
â”œâ”€â”€ README.md           # DocumentaciÃ³n del proyecto
â””â”€â”€ .gitignore          # Reglas estrictas de exclusiÃ³n
```

## ğŸ›  InstalaciÃ³n y Uso

### Prerrequisitos
- Python 3.8+
- NumPy

### ConfiguraciÃ³n
1. Clona el repositorio:
   ```bash
   git clone https://github.com/juanmanuelpriet/Mytorch.git
   cd Mytorch
   ```
2. (Opcional) Crea un entorno virtual:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

### Ejemplo BÃ¡sico
```python
from mytorch.nn.linear import Linear
from mytorch.nn.activation import ReLU

# Definir una capa simple
capa = Linear(128, 64)
activacion = ReLU()

# Pase hacia adelante
salida = activacion(capa(input_tensor))
```

## ğŸ§ª Pruebas

La infraestructura de pruebas estÃ¡ optimizada para verificaciÃ³n local.
- Pendiente: ImplementaciÃ³n de suite de pruebas estÃ¡ndar.
- VerificaciÃ³n vÃ­a scripts de validaciÃ³n locales.

## ğŸ—º Hoja de Ruta (Roadmap)

- [x] Capas Lineales y de ActivaciÃ³n bÃ¡sicas.
- [x] NormalizaciÃ³n por Lotes (Batch Normalization).
- [x] Arquitectura MLP.
- [ ] Implementar optimizadores avanzados (Adam, RMSProp).
- [ ] Soporte para Capas Convolucionales (CNNs).
- [ ] DocumentaciÃ³n avanzada.

---
*Desarrollado con fines educativos para el curso de Fundamentos de Redes Neuronales.*
