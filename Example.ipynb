{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üî¨ Informe de Investigaci√≥n: Convergencia de Redes Neuronales en MyTorch\n",
                "\n",
                "Este cuadernillo presenta un an√°lisis exhaustivo y comparativo de la capacidad de aprendizaje de modelos tipo MLP (Perceptr√≥n Multicapa) utilizando la librer√≠a `mytorch` implementada desde cero con NumPy.\n",
                "\n",
                "## Objetivos del Experimento\n",
                "1.  **Benchmarking de Optimizadores**: Evaluar la velocidad y estabilidad de SGD, Momentum y Adam.\n",
                "2.  **Impacto de la Activaci√≥n**: Comparar la efectividad de ReLU, GeLU, Sigmoid y Tanh en la formaci√≥n de fronteras de decisi√≥n.\n",
                "3.  **Complejidad del Dataset**: Analizar el comportamiento del modelo frente a variedades topol√≥gicas (lunas, c√≠rculos, espirales).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Configuraci√≥n y Reproducibilidad\n",
                "Para garantizar que los experimentos sean cient√≠ficos y comparables, fijamos una semilla global y cargamos las dependencias necesarias."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import sys\n",
                "import os\n",
                "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
                "\n",
                "# Semilla de Reproducibilidad Solicitada\n",
                "SEED = 245573\n",
                "np.random.seed(SEED)\n",
                "\n",
                "# Asegurar importaci√≥n de la librer√≠a local\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "from mytorch.nn import Sequential, Linear, GeLU, ReLU, Sigmoid, Tanh, CrossEntropyLoss\n",
                "from mytorch.optim import SGD, Adam\n",
                "\n",
                "print(f\"‚úÖ Entorno listo. Semilla fijada en {SEED}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Utilidades de Generaci√≥n de Datos\n",
                "Generamos diversos paisajes 2D para desafiar las capacidades de representaci√≥n de la red."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_spiral_dataset(n_samples=500, noise=0.2):\n",
                "    \"\"\"Genera un dataset de espiral 2D no lineal.\"\"\"\n",
                "    n = np.sqrt(np.random.rand(n_samples, 1)) * 780 * (2 * np.pi) / 360\n",
                "    d1x = -np.cos(n) * n + np.random.randn(n_samples, 1) * noise\n",
                "    d1y = np.sin(n) * n + np.random.randn(n_samples, 1) * noise\n",
                "    X = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n",
                "    y = np.hstack((np.zeros(n_samples), np.ones(n_samples))).astype(int)\n",
                "    return X, y\n",
                "\n",
                "def get_datasets():\n",
                "    \"\"\"Retorna un diccionario con todos los datasets de prueba.\"\"\"\n",
                "    moons = make_moons(n_samples=400, noise=0.15, random_state=SEED)\n",
                "    circles = make_circles(n_samples=400, noise=0.1, factor=0.5, random_state=SEED)\n",
                "    blobs = make_blobs(n_samples=400, centers=2, cluster_std=2.0, random_state=SEED)\n",
                "    spiral = generate_spiral_dataset(n_samples=250, noise=0.3)\n",
                "    noisy = make_moons(n_samples=400, noise=0.4, random_state=SEED) # Alta varianza\n",
                "    \n",
                "    return {\n",
                "        \"Lunas\": moons,\n",
                "        \"C√≠rculos\": circles,\n",
                "        \"Blobs\": blobs,\n",
                "        \"Espiral\": spiral,\n",
                "        \"Ruido Alto\": noisy\n",
                "    }\n",
                "\n",
                "# Visualizaci√≥n previa de los retos\n",
                "datasets = get_datasets()\n",
                "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
                "for i, (name, (X, y)) in enumerate(datasets.items()):\n",
                "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='Spectral', edgecolors='k', s=20)\n",
                "    axes[i].set_title(name)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† Motor de Entrenamiento Experimental\n",
                "Implementamos una funci√≥n de entrenamiento que captura m√©tricas detalladas y estados intermedios para animaciones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_with_metrics(model, optimizer, criterion, X, y, epochs=300):\n",
                "    Y_oh = np.eye(2)[y]\n",
                "    history = {\"loss\": [], \"accuracy\": [], \"time_per_epoch\": [], \"boundaries\": []}\n",
                "    \n",
                "    start_total = time.time()\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        t0 = time.time()\n",
                "        \n",
                "        # Forward\n",
                "        out = model.forward(X)\n",
                "        loss = criterion.forward(out, Y_oh)\n",
                "        \n",
                "        # Accuracy\n",
                "        preds = np.argmax(out, axis=1)\n",
                "        acc = np.mean(preds == y)\n",
                "        \n",
                "        # Backward\n",
                "        grad = criterion.backward()\n",
                "        model.backward(grad)\n",
                "        \n",
                "        # Update\n",
                "        optimizer.step()\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        t1 = time.time()\n",
                "        \n",
                "        history[\"loss\"].append(loss)\n",
                "        history[\"accuracy\"].append(acc)\n",
                "        history[\"time_per_epoch\"].append(t1 - t0)\n",
                "        \n",
                "        # Capturar frontera cada 10 √©pocas para animaci√≥n\n",
                "        if epoch % 10 == 0:\n",
                "            history[\"boundaries\"].append((epoch, get_boundary_grid(model, X)))\n",
                "            \n",
                "    history[\"total_time\"] = time.time() - start_total\n",
                "    return history\n",
                "\n",
                "def get_boundary_grid(model, X, res=0.1):\n",
                "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
                "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, res), np.arange(y_min, y_max, res))\n",
                "    Z = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = np.argmax(Z, axis=1)\n",
                "    return xx, yy, Z.reshape(xx.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Experimento 1: Comparativa de Optimizadores\n",
                "Utilizaremos el dataset de 'Lunas' como base estable para comparar c√≥mo convergen SGD, SGD con Momentum y Adam."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_optimizer_benchmark(X, y, activation_cls=ReLU, epochs=250):\n",
                "    # Definir modelos id√©nticos\n",
                "    def create_model():\n",
                "        return Sequential(\n",
                "            Linear(2, 16), activation_cls(),\n",
                "            Linear(16, 8), activation_cls(),\n",
                "            Linear(8, 2)\n",
                "        )\n",
                "    \n",
                "    results = {}\n",
                "    \n",
                "    # Configuraci√≥n de optimizadores\n",
                "    opts_config = [\n",
                "        (\"SGD\", lambda m: SGD(m, lr=0.1, momentum=0)),\n",
                "        (\"Momentum\", lambda m: SGD(m, lr=0.1, momentum=0.9)),\n",
                "        (\"Adam\", lambda m: Adam(m, lr=0.01))\n",
                "    ]\n",
                "    \n",
                "    for name, opt_fn in opts_config:\n",
                "        print(f\"Entrenando con {name}...\")\n",
                "        model = create_model()\n",
                "        optimizer = opt_fn(model)\n",
                "        history = train_with_metrics(model, optimizer, CrossEntropyLoss(), X, y, epochs)\n",
                "        results[name] = history\n",
                "        \n",
                "    return results\n",
                "\n",
                "def plot_comparison(results, title=\"Comparativa de Optimizadores\"):\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
                "    \n",
                "    for name, hist in results.items():\n",
                "        ax1.plot(hist[\"loss\"], label=name)\n",
                "        ax2.plot(hist[\"accuracy\"], label=name)\n",
                "        print(f\"‚è±Ô∏è {name} -> Tiempo Total: {hist['total_time']:.4f}s | Promedio/Epoch: {np.mean(hist['time_per_epoch']):.6f}s\")\n",
                "        \n",
                "    ax1.set_title(f\"{title} - P√©rdida\")\n",
                "    ax1.set_yscale('log')\n",
                "    ax1.legend(); ax1.grid(True)\n",
                "    \n",
                "    ax2.set_title(f\"{title} - Precisi√≥n\")\n",
                "    ax2.legend(); ax2.grid(True)\n",
                "    plt.show()\n",
                "\n",
                "X, y = datasets[\"Lunas\"]\n",
                "opt_results = run_optimizer_benchmark(X, y)\n",
                "plot_comparison(opt_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìù An√°lisis de Convergencia (Optimizadores)\n",
                "\n",
                "1.  **Adam**: Suele presentar la convergencia m√°s r√°pida gracias a su tasa de aprendizaje adaptativa por par√°metro y el uso de momentos de primer y segundo orden. Ideal para superficies de p√©rdida complejas.\n",
                "2.  **Momentum**: Supera a SGD vainilla al evitar oscilaciones en direcciones de alta curvatura, \"acumulando velocidad\" hacia el m√≠nimo global.\n",
                "3.  **SGD**: Es el m√°s lento y susceptible a quedarse atrapado en m√≠nimos locales o ser inestable si el `lr` no est√° perfectamente ajustado."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß™ Experimento 2: Impacto de las Funciones de Activaci√≥n\n",
                "Compararemos c√≥mo cambia la forma de la frontera de decisi√≥n al usar distintas no-linealidades, manteniendo Adam como optimizador constante."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_activation_benchmark(X, y, epochs=300):\n",
                "    activations = {\n",
                "        \"ReLU\": ReLU,\n",
                "        \"GeLU\": GeLU,\n",
                "        \"Sigmoid\": Sigmoid,\n",
                "        \"Tanh\": Tanh\n",
                "    }\n",
                "    \n",
                "    results = {}\n",
                "    for name, act_cls in activations.items():\n",
                "        print(f\"Probando Activaci√≥n: {name}...\")\n",
                "        model = Sequential(\n",
                "            Linear(2, 16), act_cls(),\n",
                "            Linear(16, 8), act_cls(),\n",
                "            Linear(8, 2)\n",
                "        )\n",
                "        optimizer = Adam(model, lr=0.01)\n",
                "        history = train_with_metrics(model, optimizer, CrossEntropyLoss(), X, y, epochs)\n",
                "        results[name] = history\n",
                "        \n",
                "    return results\n",
                "\n",
                "X_circ, y_circ = datasets[\"C√≠rculos\"]\n",
                "act_results = run_activation_benchmark(X_circ, y_circ)\n",
                "plot_comparison(act_results, \"Impacto de la No-Linealidad\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìù An√°lisis de No-Linealidad\n",
                "\n",
                "*   **ReLU/GeLU**: Evitan el desvanecimiento del gradiente en regiones positivas, permitiendo entrenamientos m√°s profundos y r√°pidos.\n",
                "*   **Sigmoid/Tanh**: Pueden sufrir de saturaci√≥n (gradientes casi cero) cuando las entradas son muy grandes o peque√±as, ralentizando la convergencia en redes de varias capas.\n",
                "*   **GeLU**: Introduce una curvatura suave que a menudo ayuda a la generalizaci√≥n mejor que la ReLU abrupta."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üå™Ô∏è El Reto Final: La Espiral y Animaci√≥n\n",
                "La espiral es un dataset notoriamente dif√≠cil por su entrelazamiento no lineal. Visualizaremos la evoluci√≥n de la frontera de decisi√≥n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_sp, y_sp = datasets[\"Espiral\"]\n",
                "\n",
                "# Modelo potente para la espiral\n",
                "model_sp = Sequential(\n",
                "    Linear(2, 32), GeLU(),\n",
                "    Linear(32, 16), GeLU(),\n",
                "    Linear(16, 2)\n",
                ")\n",
                "optimizer_sp = Adam(model_sp, lr=0.01)\n",
                "history_sp = train_with_metrics(model_sp, optimizer_sp, CrossEntropyLoss(), X_sp, y_sp, epochs=500)\n",
                "\n",
                "# Visualizar evoluci√≥n\n",
                "boundaries = history_sp[\"boundaries\"]\n",
                "n_snapshots = len(boundaries)\n",
                "cols = 4\n",
                "rows = (n_snapshots + cols - 1) // cols\n",
                "\n",
                "fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, (epoch, (xx, yy, Z)) in enumerate(boundaries):\n",
                "    axes[i].contourf(xx, yy, Z, alpha=0.5, cmap='Spectral')\n",
                "    axes[i].scatter(X_sp[:, 0], X_sp[:, 1], c=y_sp, s=10, cmap='Spectral')\n",
                "    axes[i].set_title(f\"√âpoca {epoch}\")\n",
                "\n",
                "for i in range(n_snapshots, len(axes)): axes[i].axis('off')\n",
                "plt.suptitle(\"Evoluci√≥n de la Frontera de Decisi√≥n (Espiral)\", fontsize=20)\n",
                "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì Conclusiones T√©cnicas\n",
                "\n",
                "1.  **Adam es el est√°ndar**: En casi todos los datasets complejos, Adam alcanz√≥ una precisi√≥n superior en menos tiempo que SGD.\n",
                "2.  **Las funciones de activaci√≥n modernas importan**: ReLU y GeLU demostraron ser fundamentales para evitar el estancamiento de la red.\n",
                "3.  **Representaci√≥n Visual**: Las animaciones muestran c√≥mo la red comienza con fronteras lineales simples y gradualmente \"tuerce\" el espacio para adaptarse a topolog√≠as complejas como la espiral.\n",
                "4.  **Reproducibilidad**: Gracias a la fijaci√≥n de la semilla `245573`, este experimento es completamente reproducible, permitiendo verificar cada m√©trica reportada."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }